# -*- coding: utf-8 -*-
"""Airbnb Review Summarization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNGfO-DnBIt_HYpp2z4VYtxrt9FNNlD3
"""

# Load necessary libraries
import pandas as pd
import nltk
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from bs4 import BeautifulSoup
import string
from transformers import BertTokenizer, BertModel, pipeline
from torch import nn
import torch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy
from collections import defaultdict

# Load dataset
df = pd.read_csv('reviews.csv')

# Basic info
df.head()

print(df.info())

df.shape

df = df.iloc[:100,:]

# Text Preprocessing: Lowercase, remove stopwords, and punctuation
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

# Data Preprocessing
df['date'] = pd.to_datetime(df['date'])  # Convert date
df['comments'] = df['comments'].fillna('')  # Fill missing comments with empty strings

def preprocess_text(text):
    text = BeautifulSoup(text,"html.parser")
    text = text.get_text().lower()  # Lowercase
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords and non-alphabetic tokens
    return ' '.join(tokens)

df['clean_comments'] = df['comments'].apply(preprocess_text)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)

# Convert text to vectors using BERT embeddings
def bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Move input tensors to the GPU if available
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state.mean(1).cpu().squeeze().numpy()

# Apply BERT to clean comments
df['embeddings'] = df['clean_comments'].apply(bert_embedding)

# Load the pre-trained sentiment analysis pipeline
bert_sentiment_analyzer = pipeline('sentiment-analysis', device=0 if torch.cuda.is_available() else -1)

# Apply the BERT-based sentiment analysis to clean comments
def analyze_sentiment(text):
    result = bert_sentiment_analyzer(text)[0]
    return result['label'], result['score']


# Apply to the dataset
df[['sentiment_label', 'sentiment_score']] = df['clean_comments'].apply(lambda x: pd.Series(analyze_sentiment(x)))

# Display results
df[['comments', 'sentiment_label', 'sentiment_score']].head()

# Load a pre-trained summarization model (e.g., BART)
# summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=0 if torch.cuda.is_available() else -1)
summarizer = pipeline('summarization', model='t5-small', device=0 if torch.cuda.is_available() else -1)
# Apply summarization to comments
def summarize_review(text):
    # We summarize if the review has more than 50 words (you can adjust as needed)
    if len(text.split()) > 50:
        summary = summarizer(text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']
        return summary
    return text

# Apply summarization to the dataset
df['summary'] = df['comments'].apply(summarize_review)

# Display the summarized reviews
df[['comments', 'summary']].head()

# Use TF-IDF to identify key topics
tfidf = TfidfVectorizer(max_features=1000)
tfidf_matrix = tfidf.fit_transform(df['clean_comments'])
feature_names = tfidf.get_feature_names_out()

def get_top_keywords(doc, n=10):
    # Get the TF-IDF scores for the document
    sorted_indices = doc.toarray()[0].argsort()[::-1]  # Sort indices by score
    top_indices = sorted_indices[:n]  # Get top N indices
    top_keywords = [feature_names[i] for i in top_indices]
    return top_keywords

# Get top 10 keywords for each document
df['key_topics'] = [get_top_keywords(doc) for doc in tfidf_matrix]

# Display comments along with the top 10 keywords
df[['comments', 'key_topics']].head()

# Plot Sentiment Distribution
sns.countplot(x='sentiment_label', data=df)
plt.title('Sentiment Distribution')
plt.show()

# Word Cloud for most common words
from wordcloud import WordCloud

text = ' '.join(df['clean_comments'])
wordcloud = WordCloud(background_color='white').generate(text)

plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

df[['comments', 'clean_comments', 'sentiment_label', 'sentiment_score', 'summary', 'key_topics']].head()

print("Review: ", df['comments'][0])
print()
print("Cleaned Review: ", df['clean_comments'][0])
print()
print("Sentiment Label: ", df['sentiment_label'][0])
print()
print("Sentiment Score: ", df['sentiment_score'][0])
print()
print("Summary: ", df['summary'][0])
print()
print("Topics: ", df['key_topics'][0])
print()

